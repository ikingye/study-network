'use strict';(function(){const t={cache:!0};t.doc={id:"id",field:["title","content"],store:["title","href","section"]};const e=FlexSearch.create("balance",t);window.bookSearchIndex=e,e.add({id:0,href:"/study-network/docs/basic/",title:"第一部分 基础入门",section:"Docs",content:"服务器测试网速 #  sivel/speedtest-cli Command line interface for testing internet bandwidth using speedtest.net\n安装：\npip install speedtest-cli  # 或者直接下载 wget -O speedtest-cli https://raw.githubusercontent.com/sivel/speedtest-cli/master/speedtest.py chmod +x speedtest-cli 示例：\n$ ./speedtest.py Retrieving speedtest.net configuration... Testing from CNISP Group (36.110.199.49)... Retrieving speedtest.net server list... Selecting best server based on ping... Hosted by 江苏电信5G (Nanjing) [1.81 km]: 51.713 ms Testing download speed................................................................................ Download: 1063.20 Mbit/s Testing upload speed................................................................................................ Upload: 149.86 Mbit/s "}),e.add({id:1,href:"/study-network/docs/advanced/code/spec/",title:"编程规范",section:"2.1 网络编程",content:"编程规范 #  "}),e.add({id:2,href:"/study-network/docs/basic/cmd/",title:"1.1 命令",section:"第一部分 基础入门",content:"命令 #  "}),e.add({id:3,href:"/study-network/docs/advanced/code/",title:"2.1 网络编程",section:"第二部分 进阶实战",content:"网络编程 #  "}),e.add({id:4,href:"/study-network/docs/design/stack/",title:"3.1 协议栈",section:"第三部分 设计与实现",content:"协议栈 #  "}),e.add({id:5,href:"/study-network/docs/design/code/netty-4.0.33/",title:"4.0.33",section:"3.7 源码分析",content:"Netty 4.0.33 源码分析 #  参考 #  yongshun/learn_netty_source_code #  "}),e.add({id:6,href:"/study-network/docs/appendix/tutorial/",title:"4.1 教程",section:"第四部分 附录",content:"教程 #  基础 #  进阶 #  tonydeng/sdn-handbook #  SDN 手册\n"}),e.add({id:7,href:"/study-network/docs/basic/cmd/curl/",title:"curl",section:"1.1 命令",content:"curl #  curl format #  curl -w @curl-format -o /dev/null -s http://www.baidu.com\n time_appconnect: %{time_appconnect} （从开始到ssl/ssh连接完成）\\n  time_connect: %{time_connect} （从开始到tcp建连完成）\\n  time_namelookup: %{time_namelookup} （从开始到域名解析完成）\\n  time_pretransfer: %{time_pretransfer} （从开始到文件传输即将开始，不包括服务器处理时间）\\n  time_redirect: %{time_redirect} （包括最后一次传输前的所有重定向，包括DNS解析，连接，预传输）\\n time_starttransfer: %{time_starttransfer} （从开始到第一个byte即将传输，包括服务器处理时间）\\n  ----------\\n  DNS 解析耗时: %{time_namelookup} \\n  tcp 建连耗时: %{time_connect} - %{time_namelookup} \\n  ssl 耗时: %{time_appconnect} - %{time_connect} \\n  服务器处理耗时: %{time_starttransfer} - %{time_pretransfer} \\n  ----------\\n  time_total: %{time_total} （全部时间；以上单位都是 秒）\\n\\n  详细参考: https://curl.haxx.se/docs/manpage.html \\n "}),e.add({id:8,href:"/study-network/docs/advanced/code/epoll/",title:"epoll",section:"2.1 网络编程",content:"epoll #  阻塞 #   多进程 多线程  非阻塞 #  忙轮询 #  while true {  for i in 流[] {  if i has 数据 {  读 或者 其他处理  }  } } select #  select 代收员 比较懒，她只会告诉你快递到了，但是是谁到的，你需要挨个快递员问一遍。\nwhile true {  select(流[]); # 阻塞   for i in 流[] {  if i has 数据 {  读 或者 其他处理  }  } } epoll #  while true {  可处理的流[] = epoll_wait(epoll_fd); # 阻塞   for i in 可处理的流[] {  读 或者 其他处理  } } epoll 编程框架 #  //创建 epoll int epfd = epoll_crete(1000);  //将 listen_fd 添加进 epoll 中 epoll_ctl(epfd, EPOLL_CTL_ADD, listen_fd, \u0026amp;listen_event);  while (1) {  //阻塞等待 epoll 中 的 fd 触发  int active_cnt = epoll_wait(epfd, events, 1000, -1);   for (i = 0 ; i \u0026lt; active_cnt; i++) {  if (evnets[i].data.fd == listen_fd) {  //accept. 并且将新 accept 的 fd 加进 epoll 中.  }  else if (events[i].events \u0026amp; EPOLLIN) {  //对此 fd 进行读操作  }  else if (events[i].events \u0026amp; EPOLLOUT) {  //对此 fd 进行写操作  }  } } 服务端 #  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;ctype.h\u0026gt;#include \u0026lt;string.h\u0026gt; #include \u0026lt;unistd.h\u0026gt;#include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt; #include \u0026lt;sys/epoll.h\u0026gt;  #define SERVER_PORT (7778) #define EPOLL_MAX_NUM (2048) #define BUFFER_MAX_LEN (4096)  char buffer[BUFFER_MAX_LEN];  void str_toupper(char *str) {  int i;  for (i = 0; i \u0026lt; strlen(str); i ++) {  str[i] = toupper(str[i]);  } }  int main(int argc, char **argv) {  int listen_fd = 0;  int client_fd = 0;  struct sockaddr_in server_addr;  struct sockaddr_in client_addr;  socklen_t client_len;   int epfd = 0;  struct epoll_event event, *my_events;   // socket  listen_fd = socket(AF_INET, SOCK_STREAM, 0);   // bind  server_addr.sin_family = AF_INET;  server_addr.sin_addr.s_addr = htonl(INADDR_ANY);  server_addr.sin_port = htons(SERVER_PORT);  bind(listen_fd, (struct sockaddr*)\u0026amp;server_addr, sizeof(server_addr));   // listen  listen(listen_fd, 10);   // epoll create  epfd = epoll_create(EPOLL_MAX_NUM);  if (epfd \u0026lt; 0) {  perror(\u0026#34;epoll create\u0026#34;);  goto END;  }   // listen_fd -\u0026gt; epoll  event.events = EPOLLIN;  event.data.fd = listen_fd;  if (epoll_ctl(epfd, EPOLL_CTL_ADD, listen_fd, \u0026amp;event) \u0026lt; 0) {  perror(\u0026#34;epoll ctl add listen_fd \u0026#34;);  goto END;  }   my_events = malloc(sizeof(struct epoll_event) * EPOLL_MAX_NUM);    while (1) {  // epoll wait  int active_fds_cnt = epoll_wait(epfd, my_events, EPOLL_MAX_NUM, -1);  int i = 0;  for (i = 0; i \u0026lt; active_fds_cnt; i++) {  // if fd == listen_fd  if (my_events[i].data.fd == listen_fd) {  //accept  client_fd = accept(listen_fd, (struct sockaddr*)\u0026amp;client_addr, \u0026amp;client_len);  if (client_fd \u0026lt; 0) {  perror(\u0026#34;accept\u0026#34;);  continue;  }   char ip[20];  printf(\u0026#34;new connection[%s:%d]\\n\u0026#34;, inet_ntop(AF_INET, \u0026amp;client_addr.sin_addr, ip, sizeof(ip)), ntohs(client_addr.sin_port));   event.events = EPOLLIN | EPOLLET;  event.data.fd = client_fd;  epoll_ctl(epfd, EPOLL_CTL_ADD, client_fd, \u0026amp;event);  }  else if (my_events[i].events \u0026amp; EPOLLIN) {  printf(\u0026#34;EPOLLIN\\n\u0026#34;);  client_fd = my_events[i].data.fd;   // do read   buffer[0] = \u0026#39;\\0\u0026#39;;  int n = read(client_fd, buffer, 5);  if (n \u0026lt; 0) {  perror(\u0026#34;read\u0026#34;);  continue;  }  else if (n == 0) {  epoll_ctl(epfd, EPOLL_CTL_DEL, client_fd, \u0026amp;event);  close(client_fd);  }  else {  printf(\u0026#34;[read]: %s\\n\u0026#34;, buffer);  buffer[n] = \u0026#39;\\0\u0026#39;; #if 1  str_toupper(buffer);  write(client_fd, buffer, strlen(buffer));  printf(\u0026#34;[write]: %s\\n\u0026#34;, buffer);  memset(buffer, 0, BUFFER_MAX_LEN); #endif  /* event.events = EPOLLOUT; event.data.fd = client_fd; epoll_ctl(epfd, EPOLL_CTL_MOD, client_fd, \u0026amp;event); */  }  }  else if (my_events[i].events \u0026amp; EPOLLOUT) {  printf(\u0026#34;EPOLLOUT\\n\u0026#34;); /* client_fd = my_events[i].data.fd; str_toupper(buffer); write(client_fd, buffer, strlen(buffer)); printf(\u0026#34;[write]: %s\\n\u0026#34;, buffer); memset(buffer, 0, BUFFER_MAX_LEN); event.events = EPOLLIN; event.data.fd = client_fd; epoll_ctl(epfd, EPOLL_CTL_MOD, client_fd, \u0026amp;event); */  }  }  }    END:  close(epfd);  close(listen_fd);  return 0; } 客户端 #  #include \u0026lt;stdio.h\u0026gt;#include \u0026lt;stdlib.h\u0026gt;#include \u0026lt;string.h\u0026gt;#include \u0026lt;strings.h\u0026gt; #include \u0026lt;sys/types.h\u0026gt;#include \u0026lt;sys/socket.h\u0026gt;#include \u0026lt;arpa/inet.h\u0026gt;#include \u0026lt;unistd.h\u0026gt;#include \u0026lt;fcntl.h\u0026gt; #define MAX_LINE (1024) #define SERVER_PORT (7778)  void setnoblocking(int fd) {  int opts = 0;  opts = fcntl(fd, F_GETFL);  opts = opts | O_NONBLOCK;  fcntl(fd, F_SETFL); }  int main(int argc, char **argv) {  int sockfd;  char recvline[MAX_LINE + 1] = {0};   struct sockaddr_in server_addr;   if (argc != 2) {  fprintf(stderr, \u0026#34;usage ./client \u0026lt;SERVER_IP\u0026gt;\\n\u0026#34;);  exit(0);  }    // 创建socket  if ( (sockfd = socket(AF_INET, SOCK_STREAM, 0)) \u0026lt; 0) {  fprintf(stderr, \u0026#34;socket error\u0026#34;);  exit(0);  }    // server addr 赋值  bzero(\u0026amp;server_addr, sizeof(server_addr));  server_addr.sin_family = AF_INET;  server_addr.sin_port = htons(SERVER_PORT);   if (inet_pton(AF_INET, argv[1], \u0026amp;server_addr.sin_addr) \u0026lt;= 0) {  fprintf(stderr, \u0026#34;inet_pton error for %s\u0026#34;, argv[1]);  exit(0);  }    // 链接服务端  if (connect(sockfd, (struct sockaddr*) \u0026amp;server_addr, sizeof(server_addr)) \u0026lt; 0) {  perror(\u0026#34;connect\u0026#34;);  fprintf(stderr, \u0026#34;connect error\\n\u0026#34;);  exit(0);  }   setnoblocking(sockfd);   char input[100];  int n = 0;  int count = 0;     // 不断的从标准输入字符串  while (fgets(input, 100, stdin) != NULL)  {  printf(\u0026#34;[send] %s\\n\u0026#34;, input);  n = 0;  // 把输入的字符串发送 到 服务器中去  n = send(sockfd, input, strlen(input), 0);  if (n \u0026lt; 0) {  perror(\u0026#34;send\u0026#34;);  }   n = 0;  count = 0;    // 读取 服务器返回的数据  while (1)  {  n = read(sockfd, recvline + count, MAX_LINE);  if (n == MAX_LINE)  {  count += n;  continue;  }  else if (n \u0026lt; 0){  perror(\u0026#34;recv\u0026#34;);  break;  }  else {  count += n;  recvline[count] = \u0026#39;\\0\u0026#39;;  printf(\u0026#34;[recv] %s\\n\u0026#34;, recvline);  break;  }  }  }   return 0; } 参考 #   Libevent 深入浅出  "}),e.add({id:9,href:"/study-network/docs/basic/cmd/iptables/",title:"iptables",section:"1.1 命令",content:"iptables #  Linux 的包过滤功能，即 Linux 防火墙， 它由 netfilter 和 iptables 两个组件组成。\nnetfilter 组件位于内核空间，是内核的一部分，由一些信息包过滤表组成，这些表包含内核用来控制信息包过滤处理的规则集。\niptables 组件是一种工具，位于用户空间，它使插入、修改和除去信息包过滤表中的规则变得容易。\n iptables 是 linux 系统下用来配置 netfilter 子系统的一个 client tool。\niptables 其实不是真正的防火墙，我们可以把它理解成一个客户端代理，用户通过 iptables 这个代理，将用户的安全设定执行到对应的 \u0026ldquo;安全框架\u0026rdquo; 中， 这个 \u0026ldquo;安全框架\u0026rdquo; 才是真正的防火墙，这个框架的名字叫 netfilter。\n配套的常用命令有：\n iptables: 对表、链、规则进行配置 iptables-save: dump 已配置的规则，可以用 \u0026gt; 重定向到一个文件中 iptables-restore: 从之前导出的 iptable 规则配置文件加载规则。   原理 #   一个数据包进入网卡时，它首先进入 PREROUTING 链，内核根据数据包目的 IP 判断是否需要转发出去 如果数据包就是进入本机的，它就会沿着图向下移动，到达 INPUT 链 数据包到了 INPUT 链后，任何进程都会收到它。 如果数据包是要转发出去的，且内核允许转发，数据包就会如图所示向右移动，经过 FORWARD 链， 然后到达 POSTROUTING 链输出。 本机上运行的程序可以发送数据包，这些数据包会经 过 OUTPUT 链 然后到达 POSTROUTING 链输出   状态 #  ESTABLISHED #  状态 ESTABLISHED 指出该信息包属于已建立的连接，该连接一直用于发送和接收信息包并且完全有效。\nINVALID #  INVALID 状态指出该信息包与任何已知的流或连接都不相关联，它可能包含错误的数据或头。\nNEW #  状态 NEW 意味着该信息包已经或将启动新的连接，或者它与尚未用于发送和接收信息包的连接相关联。\nRELATED #  RELATED 表示该信息包正在启动新连接，以及它与已建立的连接相关联。\n 规则 /rules #   规则（rules）其实就是网络管理员预定义的条件， 规则一般的定义为 \u0026ldquo;如果数据包头符合这样的条件，就这样处理这个数据包\u0026rdquo;。 规则存储在内核空间的信息包过滤表中， 这些规则分别指定了**源地址、目的地址、传输协议（如 TCP、UDP、ICMP）和服务类型（如 HTTP、FTP 和 SMTP）**等。 当数据包与规则匹配时，iptables 就根据规则所定义的方法来处理这些数据包，如放行（accept）、拒绝（reject）和丢弃（drop）等。 配置防火墙的主要工作就是添加、修改和删除这些规则。   表 /tables #  这 5 张表的优先级从高到低是：raw、mangle、nat、filter、security。\n iptables 的链（chain）对应 netfilter 的 5 处钩子 iptables 的规则就是挂在 netfilter 钩子上的函数，用来修改数据包的内容或过滤数据包 iptables 的表就是所有规则的 5 个逻辑集合  iptables 不支持用户自定义表。\nRaw #    RAW 表 只使用在 PREROUTING 链和 OUTPUT 链上，因为优先级最高，\n  从而可以对收到的数据包在连接跟踪前进行处理。\n  一但用户使用了 RAW 表，在 某个链上，RAW 表处理完后，将跳过 NAT 表和 ip_conntrack 处理，\n  即不再做地址转换和数据包的链接跟踪处理了.\n  Mangle #   主要用于对指定数据包进行更改， 在内核版本 2.4.18 后的 linux 版本中该表包含的链为：INPUT 链（处理进入的数据包），RORWARD 链（处理转发的数据包），OUTPUT 链（处理本地生成的数据包）POSTROUTING 链（修改即将出去的数据包），PREROUTING 链（修改即将到来的数据包）  NAT #   主要用于网络地址转换 NAT，该表可以实现一对一，一对多，多对多等 NAT 工作， iptables 就是使用该表实现共享上网的， NAT 表包含了 PREROUTING 链（修改即将到来的数据包），POSTROUTING 链（修改即将出去的数据包），OUTPUT 链（修改路由之前本地生成的数据包）  Filter #    Filter 表是默认的表，\n  如果没有指定哪个表，iptables 就默认使用 filter 表来执行所有命令，\n  filter 表包含了 INPUT 链（处理进入的数据包），RORWARD 链（处理转发的数据包），OUTPUT 链（处理本地生成的数据包）\n  在 filter 表中只能允许对数据包进行接受，丢弃的操作，而无法对数据包进行更改\n  security #  用于强制访问控制网络规则\n表具有的链类型 #     规则名称 raw mangle nat filter security     PREROUTING ✓ ✓ ✓     INPUT  ✓ ✓ ✓ ✓   OUTPUT  ✓ ✓ ✓ ✓   POSTROUTING  ✓ ✓     FORWARD ✓ ✓  ✓ ✓     命令选项 #  命令选项用于指定 iptables 的执行方式，包括插入规则，删除规则和添加规则，如下表所示\n   短命令 长命令 说明     -P \u0026ndash;policy 定义默认策略   -L \u0026ndash;list 查看 iptables 规则列表   -A \u0026ndash;append 在规则列表的最后增加 1 条规则   -I \u0026ndash;insert 在指定的位置插入 1 条规则   -D \u0026ndash;delete 从规则列表中删除 1 条规则   -R \u0026ndash;replace 替换规则列表中的某条规则   -F \u0026ndash;flush 删除表中所有规则   -Z \u0026ndash;zero 将表中数据包计数器和流量计数器归零   -X \u0026ndash;delete-chain 删除自定义链   -v \u0026ndash;verbose 与 - L 他命令一起使用显示更多更详细的信息     链 /chains #   当一个数据包到达一个链时，iptables 就会从链中第一条规则开始检查，看该数据包是否满足规则所定义的条件。 如果满足，系统就会根据该条规则所定义的方法处理该数据包；否则 iptables 将继续检查下一条规则， 如果该数据包不符合链中任一条规则，iptables 就会根据该链预先定义的默认策略来处理数据包。  PREROUTING 链 #  内核根据数据包目的 IP 判断是否需要转发出去\nINPUT 链 #  处理来自外部的数据\nOUTPUT 链 #  处理向外发送的数据\nFORWARD 链 #  将数据转发到本机的其他网卡设备上。\nPOSTROUTING 链 #   iptables 调用链 #   条件匹配 #     短命令 长命令       -i \u0026ndash;in-interface 网络接口名 指定数据包从哪个网络接口进入   -o \u0026ndash;out-interface 网络接口名 指定数据包从哪个网络接口输出   -p \u0026mdash;proto 协议类型 指定数据包匹配的协议，如 TCP、UDP 和 ICMP 等   -s \u0026ndash;source 源地址或子网 指定数据包匹配的源地址    \u0026ndash;sport 源端口号 指定数据包匹配的源端口号    \u0026ndash;dport 目的端口号 指定数据包匹配的目的端口号   -m \u0026ndash;match 匹配的模块 指定数据包规则所使用的过滤模块     目标动作或跳转 #  ACCEPT、REJECT、DROP、REDIRECT 、MASQUERADE\n还有 LOG、ULOG、DNAT、RETURN、TOS、SNAT、MIRROR、QUEUE、TTL、MARK\n NAT #  DNAT #  公网 ip -\u0026gt; 内部地址\nSNAT #  内部地址 -\u0026gt; 公网 ip\n 保存 iptables 修改 #  方法一：service iptables save\n方法二：\n  修改 /etc/sysconfig/iptables-config\n  将里面的 IPTABLES_SAVE_ON_STOP=\u0026ldquo;no\u0026rdquo;, 改为 \u0026ldquo;yes\u0026rdquo;\n  这样每次服务在停止之前会自动将现有的规则保存在 /etc/sysconfig/iptables 这个文件中去。\n   基础命令 #  iptables [ -t 表名] 命令选项 [链名] [条件匹配] [-j 目标动作或跳转]\niptables v1.4.21  Usage: iptables -[ACD] chain rule-specification [options]  iptables -I chain [rulenum] rule-specification [options]  iptables -R chain rulenum rule-specification [options]  iptables -D chain rulenum [options]  iptables -[LS] [chain [rulenum]] [options]  iptables -[FZ] [chain] [options]  iptables -[NX] chain  iptables -E old-chain-name new-chain-name  iptables -P chain target [options]  iptables -h (print this help information)  Commands: Either long or short options are allowed.  --append -A chain	Append to chain  --check -C chain	Check for the existence of a rule  --delete -D chain	Delete matching rule from chain  --delete -D chain rulenum 	Delete rule rulenum (1 = first) from chain  --insert -I chain [rulenum] 	Insert in chain as rulenum (default 1=first)  --replace -R chain rulenum 	Replace rule rulenum (1 = first) in chain  --list -L [chain [rulenum]] 	List the rules in a chain or all chains  --list-rules -S [chain [rulenum]] 	Print the rules in a chain or all chains  --flush -F [chain]	Delete all rules in chain or all chains  --zero -Z [chain [rulenum]] 	Zero counters in chain or all chains  --new -N chain	Create a new user-defined chain  --delete-chain  -X [chain]	Delete a user-defined chain  --policy -P chain target 	Change policy on chain to target  --rename-chain  -E old-chain new-chain 	Change chain name, (moving any references) Options:  --ipv4	-4	Nothing (line is ignored by ip6tables-restore)  --ipv6	-6	Error (line is ignored by iptables-restore) [!] --protocol	-p proto	protocol: by number or name, eg. `tcp\u0026#39; [!] --source	-s address[/mask][...] source specification [!] --destination -d address[/mask][...] destination specification [!] --in-interface -i input name[+] network interface name ([+] for wildcard) --jump	-j target target for rule (may load target extension) --goto -g chain jump to chain with no return --match	-m match extended match (may load extension) --numeric	-n	numeric output of addresses and ports [!] --out-interface -o output name[+] network interface name ([+] for wildcard) --table	-t table	table to manipulate (default: `filter\u0026#39;)  --verbose	-v	verbose mode  --wait	-w [seconds]	maximum wait to acquire xtables lock before give up  --wait-interval -W [usecs]	wait time to try to acquire xtables lock 	default is 1 second  --line-numbers	print line numbers when listing  --exact	-x	expand numbers (display exact values) [!] --fragment	-f	match second or further fragments only  --modprobe=\u0026lt;command\u0026gt;	try to insert modules using this command  --set-counters PKTS BYTES	set the counter during insert/append [!] --version	-V	print package version. 默认是 filter 表\niptables -L -n  iptables -t nat -L -n  教程 #  超级详细的 iptable 教程文档 #  常见 iptables 使用规则场景整理 #  "}),e.add({id:10,href:"/study-network/docs/advanced/code/libevent/",title:"libevent",section:"2.1 网络编程",content:"libevent #  libevent/libevent Event notification library https://libevent.org\n支持多种 I/O 多路复用技术，epoll、poll、dev/poll、select 和 kqueue 等\n reactor #  教程 #   Libevent 深入浅出  "}),e.add({id:11,href:"/study-network/docs/basic/cmd/netfilter/",title:"netfilter",section:"1.1 命令",content:"netfilter #  "}),e.add({id:12,href:"/study-network/docs/advanced/code/select/",title:"select",section:"2.1 网络编程",content:"select #  "}),e.add({id:13,href:"/study-network/docs/basic/protocol/tcp/",title:"TCP",section:"1.2 协议",content:"TCP #  TCP 报文格式 #   序号：Seq 序号，占 32 位，用来标识从 TCP 源端向目的端发送的字节流，发起方发送数据时对此进行标记。 确认序号：Ack 序号，占 32 位，只有 ACK 标志位为 1 时，确认序号字段才有效，Ack=Seq+1。 标志位：共 6 个，即 URG、ACK、PSH、RST、SYN、FIN 等，具体含义如下：  URG：紧急指针（urgent pointer）有效。 ACK：确认序号有效。 PSH：接收方应该尽快将这个报文交给应用层。 RST：重置连接。 SYN：发起一个新连接。 FIN：释放一个连接。    三次握手 #  四次挥手 #  实际中还会出现同时发起主动关闭的情况\n TCP 有限状态机 #  The TCP Finite State Machine (FSM)\n\n TCP 状态变迁图 #  \n TCP 是全双工的 #   TCP 的 RTT 算法 #  经典算法（RFC793） #    首先，先采样 RTT，记下最近好几次的 RTT 值。\n  然后做平滑计算 SRTT（Smoothed RTT），公式为：\nSRTT = (α * SRTT) + ((1- α) * RTT)\n其中的 α 取值在 0.8 到 0.9 之间，这个算法英文叫 Exponential weighted moving average，中文叫：加权移动平均\n  开始计算 RTO。公式如下：\nRTO = min [UBOUND, max [LBOUND, (β * SRTT)]]\n  其中：\n UBOUND 是最大的 timeout 时间，上限值 LBOUND 是最小的 timeout 时间，下限值 β 值一般在 1.3 到 2.0 之间。  Karn / Partridge 算法 #  Jacobson / Karels 算法 #   TCP 滑动窗口 #  TCP 缓冲区的数据结构\n\n接收端在给发送端回 ACK 中会汇报自己的 AdvertisedWindow = MaxRcvBuffer – LastByteRcvd – 1;\n发送方会根据这个窗口来控制发送数据的大小，以保证接收方可以处理\n发送方的滑动窗口 #  \n上图中分成了四个部分，分别是：（其中那个黑模型就是滑动窗口）\n #1 已收到 ack 确认的数据。 #2 发还没收到 ack 的。 #3 在窗口中还没有发出的（接收方还有空间）。 #4 窗口以外的数据（接收方没空间）  接受端控制发送端的图示 #  \nTCP 的拥塞处理 #  拥塞控制主要是四个算法：\n 慢启动 拥塞避免 拥塞发生 快速恢复  慢热启动算法 – Slow Start #  慢启动的算法如下 (cwnd 全称 Congestion Window)：\n 连接建好的开始先初始化 cwnd = 1，表明可以传一个 MSS 大小的数据。 每当收到一个 ACK，cwnd++; 呈线性上升 每当过了一个 RTT，cwnd = cwnd*2; 呈指数让升 还有一个 ssthresh（slow start threshold），是一个上限，当 cwnd \u0026gt;= ssthresh 时，就会进入 “拥塞避免算法”  \n拥塞避免算法 – Congestion Avoidance #  ssthresh（slow start threshold）是一个上限， 当 cwnd \u0026gt;= ssthresh 时，就会进入 “拥塞避免算法”。\n一般来说 ssthresh 的值是 65535，单位是字节，当 cwnd 达到这个值时后，算法如下：\n 收到一个 ACK 时，cwnd = cwnd + 1/cwnd 当每过一个 RTT 时，cwnd = cwnd + 1  这样就可以避免增长过快导致网络拥塞，慢慢的增加调整到网络的最佳值。很明显，是一个线性上升的算法。\n拥塞状态时的算法 #  快速恢复算法 – Fast Recovery #  FACK 算法 #  更多拥塞控制请参考论文《Congestion Avoidance and Control》(PDF)\n SYN 攻击 #  在三次握手过程中，Server 发送 SYN-ACK 之后，收到 Client 的 ACK 之前的 TCP 连接称为半连接（half-open connect），\n此时 Server 处于 SYN_RCVD 状态，\n当收到 ACK 后，Server 转入 ESTABLISHED 状态。\nSYN 攻击就是 Client 在短时间内伪造大量不存在的 IP 地址，并向 Server 不断地发送 SYN 包，Server 回复确认包，并等待 Client 的确认， 由于源地址是不存在的，因此，Server 需要不断重发直至超时，\n这些伪造的 SYN 包将产时间占用未连接队列，导致正常的 SYN 请求因为队列满而被丢弃，从而引起网络堵塞甚至系统瘫痪。\nSYN 攻击时一种典型的 DDOS 攻击，\n检测 SYN 攻击的方式非常简单，即当 Server 上有大量半连接状态且源 IP 地址是随机的，则可以断定遭到 SYN 攻击了，\n使用如下命令可以让之现行：\n$ netstat -nap | grep SYN_RECV 于是，server 端如果在一定时间内没有收到的 TCP 会重发 SYN-ACK。 在 Linux 下，默认重试次数为 5 次， 重试的间隔时间从 1s 开始每次都翻售，5 次的重试时间间隔为 1s, 2s, 4s, 8s, 16s，总共 31s， 第 5 次发出后还要等 32s 都知道第 5 次也超时了， 所以，总共需要 1s + 2s + 4s+ 8s+ 16s + 32s = 2^6 -1 = 63s，TCP 才会把断开这个连接。\n解决 #  对于正常的请求，你应该调整三个 TCP 参数可供你选择，\n tcp_synack_retries 可以用他来减少重试次数 tcp_max_syn_backlog 可以增大 SYN 连接数 tcp_abort_on_overflow 处理不过来干脆就直接拒绝连接了   问答 #  为什么建立连接是三次握手，而关闭连接却是四次挥手呢？ #  因为服务端在 LISTEN 状态下，收到建立连接请求的 SYN 报文后，把 ACK 和 SYN 放在一个报文里发送给客户端。\n而关闭连接时，当收到对方的 FIN 报文时，仅仅表示对方不再发送数据了但是还能接收数据，己方也未必全部数据都发送给对方了，所以己方可以立即 close，也可以发送一些数据给对方后，再发送 FIN 报文给对方来表示同意现在关闭连接， 因此，己方 ACK 和 FIN 一般都会分开发送。\n对于 4 次挥手：其实你仔细看是 2 次，因为 TCP 是全双工的，所以，发送方和接收方都需要 Fin 和 Ack。只不过，有一方是被动的，所以看上去就成了所谓的 4 次挥手。如果两边同时断连接，那就会就进入到 CLOSING 状态，然后到达 TIME_WAIT 状态。\n下图是双方同时断连接的示意图\n\n单台服务器并发 TCP 连接数到底可以有多少？ #  操作系统上端口号 1024 以下是系统保留的，从 1024-65535 是用户使用的。\n由于每个 TCP 连接都要占一个端口号，所以我们最多可以有 60000 多个并发连接？\n并不是的。这个是客户端的限制，而不是服务端。\n系统用一个 4 四元组来唯一标识一个 TCP 连接：{local ip, local port, remote ip, remote port}。\n服务端实际只使用了 bind 时这一个端口，说明端口号 65535 并不是并发量的限制\n最大 tcp 连接为客户端 ip 数 × 客户端 port 数，\n对 IPV4，不考虑 ip 地址分类等因素，最大 tcp 连接数约为 2 的 32 次方（ip 数）×2 的 16 次方（port 数），\n也就是 server 端单机最大 tcp 连接数理论上约为 2 的 48 次方。\n当然实际上单机并发连接数肯定要受硬件资源（内存）、网络资源（带宽）的限制。\n文件句柄限制 #  每一个 tcp 连接都要占一个文件描述符，\n一旦这个文件描述符使用完了，新的连接到来返回给我们的错误是 “Socket/File:Can't open so many files”。\n进程限制 #  执行 ulimit -n 输出 1024，说明对于一个进程而言最多只能打开 1024 个文件\n 用户退出后失效：ulimit -n 1000000 重启后失效：编辑 /etc/security/limits.conf 文件  soft nofile 1000000 hard nofile 1000000   永久修改：编辑 /etc/rc.local，在其后添加如下内容  ulimit -SHn 1000000    全局限制 #  执行 cat /proc/sys/fs/file-nr 输出 9344 0 592026，分别为：\n 已经分配的文件句柄数， 已经分配但没有使用的文件句柄数， 最大文件句柄数。  但在 kernel 2.6 版本中第二项的值总为 0，这并不是一个错误，它实际上意味着已经分配的文件描述符无一浪费的都已经被使用了 。\n我们可以把这个数值改大些，用 root 权限修改 /etc/sysctl.conf 文件:\nfs.file-max = 1000000 net.ipv4.ip_conntrack_max = 1000000 net.ipv4.netfilter.ip_conntrack_max = 1000000 为什么服务总会抛出一个 connet reset by peer？ #  主动关闭方直接发送了一个 RST flags，而非 FIN，就终止连接了。\n 出现大量 time_wait 状态怎么办？ #  TIME_WAIT 状态也称为 2MSL 等待状态。 每个具体 TCP 实现必须选择一个报文段最大生存时间 MSL（Maximum Segment Lifetime）。 它是任何报文段被丢弃前在网络内的最长时间。\n我们知道这个时间是有限的，因为 TCP 报文段以 IP 数据报在网络内传输，而 IP 数据报则有限制其生存时间的 TTL 字段。\nRFC 793 [Postel 1981c] 指出 MSL 为 2 分钟。 然而，实现中的常用值是 30 秒，1 分钟，或 2 分钟。\n对 IP 数据报 TTL 的限制是基于跳数，而不是定时器。\nTIME_WAIT 是怎么产生的？ #  连接关闭的过程 #   主动关闭连接的一方，调用 close()；协议层发送 FIN 包 被动关闭的一方收到 FIN 包后，协议层回复 ACK；然后被动关闭的一方，进入 CLOSE_WAIT 状态；主动关闭的一方等待对方关闭，则进入 FIN_WAIT_2 状态 被动关闭的一方在完成所有数据发送后，调用 close() 操作，协议层发送 FIN 包给主动关闭的一方，等待对方的 ACK，被动关闭的一方进入 LAST_ACK 状态 主动关闭的一方收到 FIN 包，协议层回复 ACK，主动关闭连接的一方进入 TIME_WAIT 状态；而被动关闭的一方，进入 CLOSED 状态 等待 2MSL 时间，主动关闭的一方，结束 TIME_WAIT，进入 CLOSED 状态  TIME_WAIT 只会出现在主动断开连接的一方 #  大量 TIME_WAIT 造成的影响 #   在高并发短连接的 TCP 服务器上，当服务器处理完请求后立刻主动正常关闭连接。  短连接表示 “业务处理 + 传输数据的时间 远远小于 TIMEWAIT 超时的时间” 的连接 高并发可以让服务器在短时间范围内同时占用大量端口，  而端口有个 0~65535 的范围，并不是很多，刨除系统和其他服务要用的，剩下的就更少了。     在实际业务场景中，一般长连接对应的业务的并发量并不会很高 这些端口都是服务器临时分配，无法用 SO_REUSEADDR 选项解决这个问题   内存  内核里有一个保存所有连接的 hash table 不同的内核对这个 hash table 的大小设置不同，你可以通过 dmesg 命令去找到你的内核设置的大小 还有一个 hash table 用来保存所有的 bound ports，主要用于可以快速的找到一个可用的端口或者随机端口 不过占用内存很少很少。 一个 tcp socket 占用不到 4k。1 万条 TIME_WAIT 的连接，也就多消耗 1M 左右的内存   CPU  每次找到一个随机端口，还是需要遍历一遍 bound ports 的吧，这必然需要一些 CPU 时间。   源端口数量 (net.ipv4.ip_local_port_range) TIME_WAIT bucket 数量 (net.ipv4.tcp_max_tw_buckets) 文件描述符数量 (max open file)  一个 socket 占用一个文件描述符    排查 #  查看 TCP 各个状态的数量 #  netstat -ant | awk \u0026#39;/^tcp/ {++S[$NF]} END {for(a in S) print (a,S[a])}\u0026#39; | sort -rn -k2 TCP 状态含义 #     TCP 状态 含义     LISTEN 服务器在等待进入呼叫   SYN_RECV 一个连接请求已经到达，等待确认   SYN_SENT 应用已经开始，打开一个连接   ESTABLISHED 正常数据传输状态   FIN_WAIT1 应用说它已经完成   FIN_WAIT2 另一边已同意释放   ITMED_WAIT 等待所有分组死掉   LAST_ACK 等待所有分组死掉   TIME_WAIT 另一边已初始化一个释放   CLOSING 两边同时尝试关闭   CLOSED 无连接是活动的或正在进行    统计 TIME_WAIT 连接的本地地址 #  netstat -an | grep TIME_WAIT | awk \u0026#39;{print $4}\u0026#39; | sort | uniq -c | sort -rn -k1 | head 如何尽量处理 TIMEWAIT 过多的问题？ #  修改内核文件 /etc/sysctl.conf #  net.ipv4.tcp_syncookies = 1 表示开启 SYN Cookies。当出现 SYN 等待队列溢出时，启用 cookies 来处理，可防范少量 SYN 攻击，默认为 0，表示关闭； net.ipv4.tcp_tw_reuse = 1 表示开启重用。允许将 TIME-WAIT sockets 重新用于新的 TCP 连接，默认为 0，表示关闭； net.ipv4.tcp_tw_recycle = 1 表示开启 TCP 连接中 TIME-WAIT sockets 的快速回收，默认为 0，表示关闭。 net.ipv4.tcp_fin_timeout 修改系默认的 TIMEOUT 时间 然后执行 /sbin/sysctl -p 让参数生效。\n/etc/sysctl.conf 是一个允许改变正在运行中的 Linux 系统的接口，它包含一些 TCP/IP 堆栈和虚拟内存系统的高级选项，修改内核参数永久生效。\n简单来说，就是打开系统的 TIMEWAIT 重用和快速回收。\n如果以上配置调优后性能还不理想，可继续修改一下配置：\nnet.ipv4.tcp_keepalive_time = 1200 #表示当keepalive起用的时候，TCP发送keepalive消息的频度。缺省是2小时，改为20分钟。 net.ipv4.ip_local_port_range = 1024 65000 #表示用于向外连接的端口范围。缺省情况下很小：32768到61000，改为1024到65000。 net.ipv4.tcp_max_syn_backlog = 8192 #表示SYN队列的长度，默认为1024，加大队列长度为8192，可以容纳更多等待连接的网络连接数。 net.ipv4.tcp_max_tw_buckets = 5000 #表示系统同时保持 TIME_WAIT 套接字的最大数量，如果超过这个数字， TIME_WAIT 套接字将立刻被清除并打印警告信息。 默认为180000，改为5000。对于Apache、Nginx等服务器，上几行的参数可以很好地减少 TIME_WAIT 套接字数量，但是对于 Squid，效果却不大。此项参数可以控制 TIME_WAIT 套接字的最大数量，避免Squid服务器被大量的 TIME_WAIT 套接字拖死。 参考：\n 解决 TIME_WAIT 过多造成的问题   nginx 配置问题 #   nginx 在配置 \u0026ldquo;不启用 keep-alive\u0026rdquo; 时，会在 http 请求结束时主动断开连接  尝试开启 http 的 keep-alive  keepalive_timeout 65;  nginx 与 fast-cgi 的默认连接是短连接  修改 nginx 配置使其与 fastcgi 的连接使用长连接  upstream phpserver{ server 127.0.0.1:9000 weight=1; # upstream 中的 keepalive 指定 nginx 每个 worker 与 fastcgi 的最大长连接数 keepalive 100 }  若 nginx 与 fast-cgi 在同一台服务器上，则使用 unix 域 会更为高效，同时避免了 TIME_WAIT 的问题  proxy_pass 与 fastcgi_pass 的区别 #  客户端 --http--\u0026gt; 前端负载均衡 Nginx --proxy_pass--\u0026gt; 业务服务器 Nginx --fastcgi_pass--\u0026gt; 业务服务器 php-fpm 为什么要有 TIME_WAIT 状态？ #   可靠地实现 TCP 全双工连接的终止  在进行关闭连接四次挥手协议时，最后的 ACK 是由主动关闭端发出的，如果这个最终的 ACK 丢失，服务器将重发最终的 FIN 因此客户端必须维护状态信息允许它重发最终的 ACK。如果不维持这个状态信息，那么客户端将响应 RST 分节，服务器将此分节解释成一个错误（在 java 中会抛出 connection reset 的 SocketException)。 因而，要实现 TCP 全双工连接的正常终止，必须处理终止序列四个分节中任何一个分节的丢失情况，主动关闭的客户端必须维持状态信息进入 TIME_WAIT 状态。   让老的重复分节在网络中消逝  在关闭一个 TCP 连接后，马上又重新建立起一个相同的 IP 地址和端口之间的 TCP 连接，后一个连接被称为前一个连接的化身（incarnation)，那么有可能出现这种情况，前一个连接的迷途重复分组在前一个连接终止后出现，从而被误解成从属于新的化身。 为了避免这个情况， TCP 不允许处于 TIME_WAIT 状态的连接启动一个新的化身，因为 TIME_WAIT 状态持续 2MSL，就可以保证当成功建立一个 TCP 连接的时候，来自连接先前化身的重复分组已经在网络中消逝。     参考 #   TCP/IP 详解 深入理解 TCP 协议（上）：理论基础 深入理解 TCP 协议（下）：RTT、滑动窗口、拥塞处理 Wireshark 抓包分析 TCP 3 次握手、4 次挥手过程 TCP 协议的 3 次握手与 4 次挥手过程详解 不为人知的网络编程 (一)：浅析 TCP 协议中的疑难杂症 (上篇) 不为人知的网络编程 (一)：浅析 TCP 协议中的疑难杂症 (下篇) 不为人知的网络编程 (三)：关闭 TCP 连接时为什么会 TIME_WAIT、CLOSE_WAIT 高性能网络编程 (一)：单台服务器并发 TCP 连接数到底可以有多少  "}),e.add({id:14,href:"/study-network/docs/basic/protocol/vxlan/",title:"VXLAN",section:"1.2 协议",content:"VXLAN #  overlay 的核心其实就是打隧道（tunnel）\n 突破 VLAN 的最多 4096 个终端的数量限制，以满足大规模云计算数据中心的需求 解决 STP 在大型 网路设备带宽浪费和收敛性能变慢的缺陷 解决 ToR (Top of Rack) 交换机 MAC 表耗尽问题 VXLAN 最大的贡献，就是实现了网络的扁平化管理，IP 地址不再与特定的 Site（某个数据中心）强行绑定在一起。  NOV3 技术（Network Virtualization Overlays）的实现原理其实非常简单，\n VxLAN: MAC in UDP NvGRE: MAC in GRE STT: MAC in TCP（伪 TCP）  "}),e.add({id:15,href:"/study-network/docs/advanced/tool/wireshark/",title:"Wireshark",section:"2.2 网络工具",content:"Wireshark #  安装 #  https://www.wireshark.org/download.html\nyum install -y wireshark  教程 #  参考：\n Wireshark User’s Guide 命令 wireshark - Interactively dump and analyze network traffic  Capture filter #  参考：\n 4.10. Filtering while capturing Capture Filters wiki Man page of PCAP-FILTER  Display filter #  参考：\n Chapter 6. Working With Captured Packets wireshark-filter - Wireshark display filter syntax and reference  tshark #  教程：\n tshark - Dump and analyze network traffic  # 列出当前存在的网络接口 tshark -D   # 实时打印当前http请求的url $ tshark -s 512 -i eth0 -n -f \u0026#39;tcp dst port 80\u0026#39; -R \u0026#39;http.host and http.request.uri\u0026#39; -T fields -e http.host -e http.request.uri -l | tr -d \u0026#39;\\t\u0026#39; # 参数含义： # -s 512 :只抓取前512个字节数据 # -i eth0 :捕获eth0网卡 # -n :禁止网络对象名称解析 # -f \u0026#39;tcp dst port 80\u0026#39; :只捕捉协议为tcp,目的端口为80的数据包 # -R \u0026#39;http.host and http.request.uri\u0026#39; :过滤出http.host和http.request.uri # -T fields -e http.host -e http.request.uri :打印http.host和http.request.uri # -l ：输出到标准输出  # 实时打印当前mysql查询语句 $ tshark -s 512 -i eth0 -n -f \u0026#39;tcp dst port 3306\u0026#39; -R \u0026#39;mysql.query\u0026#39; -T fields -e mysql.query # 参数含义： # -s 512 :只抓取前512个字节数据 # -i eth0 :捕获eth0网卡 # -n :禁止网络对象名称解析 # -f \u0026#39;tcp dst port 3306\u0026#39; :只捕捉协议为tcp,目的端口为3306的数据包 # -R \u0026#39;mysql.query\u0026#39; :过滤出mysql.query # -T fields -e mysql.query :打印mysql查询语句 Linux 安装 Wireshark #   查询 #  ip.addr == 192.168.137.109 MacOS 打开两个 Wireshark #  # 使用 -n 可以打开多个 open -n /Applications/Wireshark.app 参考：\n Open Multiple Instances of Wireshark on MacOS  "}),e.add({id:16,href:"/study-network/docs/basic/protocol/udp/",title:"UDP",section:"1.2 协议",content:"UDP #  "}),e.add({id:17,href:"/study-network/docs/basic/protocol/",title:"1.2 协议",section:"第一部分 基础入门",content:"协议 #  "}),e.add({id:18,href:"/study-network/docs/advanced/tool/",title:"2.2 网络工具",section:"第二部分 进阶实战",content:"网络工具 #  "}),e.add({id:19,href:"/study-network/docs/appendix/interview/",title:"4.2 面试题",section:"第四部分 附录",content:"计算机网络面试题 #  基础题 #  进阶题 #  "}),e.add({id:20,href:"/study-network/docs/basic/protocol/dns/",title:"DNS",section:"1.2 协议",content:"DNS #  /etc/hosts #  hosts 文件是 Linux 系统中一个负责 IP 地址与域名快速解析的文件\n/etc/resolv.conf #  该文件是 DNS 域名解析的配置文件，它的格式很简单， 每行以一个关键字开头，后接配置参数。\nresolv.conf 的关键字主要有四个：\n nameserver # 定义 DNS 服务器的 IP 地址 domain # 定义本地域名 search # 定义域名的搜索列表 sortlist # 对返回的域名进行排序  最主要是 nameserver 关键字，如果没指定 nameserver 就找不到 DNS 服务器，其它关键字是可选的。\ndomain centoscn.com search www.centocn.com www.wgjlb.com nameserver 202.96.128.86 nameserver 202.96.128.166 /etc/host.conf #  解析器查询顺序配置文件\n# 表示先查询本地 hosts 文件，如果没有结果，再尝试查找 BIND dns 服务器 order hosts bind "}),e.add({id:21,href:"/study-network/docs/appendix/attention/",title:"4.3 关注项目",section:"第四部分 附录",content:"关注 #  其他 #  netty/netty #  Netty project - an event-driven asynchronous network application framework http://netty.io/\nlibuv/libuv #  Cross-platform asynchronous I/O https://libuv.org/\npanjf2000/gnet #  gnet 是一个基于事件驱动的高性能和轻量级网络框架。它直接使用 epoll 和 kqueue 系统调用而非标准 Go 网络包：net 来构建网络应用，它的工作原理类似两个开源的网络库：netty/netty 和 libuv/libuv 。\ngnet 设计开发的初衷不是为了取代 Go 的标准网络库：net，而是为了创造出一个类似于 antirez/redis 、haproxy/haproxy 能高效处理网络包的 Go 语言网络服务器框架。\ngnet 的卖点在于它是一个高性能、轻量级、非阻塞的纯 Go 实现的传输层（TCP/UDP/Unix Domain Socket）网络框架，开发者可以使用 gnet 来实现自己的应用层网络协议 (HTTP、RPC、Redis、WebSocket 等等)，从而构建出自己的应用层网络应用：比如在 gnet 上实现 HTTP 协议就可以创建出一个 HTTP 服务器 或者 Web 开发框架，实现 Redis 协议就可以创建出自己的 Redis 服务器等等。\ngnet 衍生自另一个项目：tidwall/evio ，但拥有更丰富的功能特性，且性能远胜之。\ntidwall/evio #  Fast event-loop networking for Go\n"}),e.add({id:22,href:"/study-network/docs/basic/protocol/http/",title:"HTTP",section:"1.2 协议",content:"HTTP #  教程 #  EZLippi/Tinyhttpd #  Tinyhttpd 是 J. David Blackstone 在 1999 年写的一个不到 500 行的超轻量型 Http Server，用来学习非常不错，可以帮助我们真正理解服务器程序的本质。 官网:http://tinyhttpd.sourceforge.net\n"}),e.add({id:23,href:"/study-network/docs/basic/protocol/ssl/",title:"SSL",section:"1.2 协议",content:"SSL #   SSL，Secure Socket Layer，安全套接字层 TLS，Transport Layer Security，传输层安全协议   版本 #   1994 年，NetScape 公司设计了 SSL 协议（Secure Sockets Layer）的 1.0 版，但是未发布。 1995 年，NetScape 公司发布 SSL 2.0 版，很快发现有严重漏洞。 1996 年，SSL 3.0 版问世，得到大规模应用。 1999 年，互联网标准化组织 ISOC 接替 NetScape 公司，发布了 SSL 的升级版 TLS 1.0 版。 2006 年和 2008 年，TLS 进行了两次升级，分别为 TLS 1.1 版和 TLS 1.2 版。最新的变动是 2018 年 TLS 1.3 的修订版。     Protocol Published Status     SSL 1.0 Unpublished Unpublished   SSL 2.0 1995 Deprecated in 2011 (RFC 6176)   SSL 3.0 1996 Deprecated in 2015 (RFC 7568)   TLS 1.0 1999 Deprecated in 2020   TLS 1.1 2006 Deprecated in 2020   TLS 1.2 2008    TLS 1.3 2018      握手流程 #   Client-hello 阶段 Server-hello 阶段   TLS #  rfc5246\nTLS 是一个信道建立和信道的表达方式，向下依托于 TCP，向上对应用程序服务。\n该协议由两层组成：\n TLS 记录协议 TLS 握手协议   DTLS #  就是同样的 TLS 逻辑被应用于 UDP 业务之上\n OpenSSL #  切换 openssl 版本 #  $ ls -l /usr/local/opt/openssl* lrwxr-xr-x 1 yewang admin 24 May 19 10:16 /usr/local/opt/openssl -\u0026gt; ../Cellar/openssl/1.0.2s lrwxr-xr-x 1 yewang admin 24 May 19 10:16 /usr/local/opt/openssl@1.0 -\u0026gt; ../Cellar/openssl/1.0.2s lrwxr-xr-x 1 yewang admin 28 May 18 20:25 /usr/local/opt/openssl@1.1 -\u0026gt; ../Cellar/openssl@1.1/1.1.1g  $ brew switch openssl 1.0.2s  mTLS #  Mutual Transport Layer Security\n 参考 #  "}),e.add({id:24,href:"/study-network/docs/basic/protocol/websocket/",title:"WebSocket",section:"1.2 协议",content:"WebSocket #  WebSocket 的目标是在一个单独的持久连接上提供全双工、双向通信。\n在 Javascript 创建了 Web Socket 之后，会有一个 HTTP 请求发送到浏览器以发起连接。 在取得服务器响应后，建立的连接会将 HTTP 升级为 WebSocket 协议。\n由于 WebSocket 使用自定义的协议，所以 URL 模式也略有不同。\n未加密的连接不再是 http://，而是 ws://; 加密的连接也不是 https://，而是 wss://。\n使用自定义协议而非 HTTP 协议的好处是，能够在客户端和服务器之间发送非常少量的数据，而不必担心 HTTP 那样字节级的开销。\n由于传递的数据包很小，所以 WebSocket 非常适合移动应用。\n特点 #   可以发送文本，也可以发送二进制数据。 没有同源限制，客户端可以与任意服务器通信。  参考：\n 推送技术 阮一峰 - WebSocket 教程  "}),e.add({id:25,href:"/study-network/docs/basic/protocol/https/",title:"HTTPs",section:"1.2 协议",content:"HTTPs #  从 HTTP 切换到 HTTPs 需要几步 #   EVALUATE YOUR WEBSITE FOR SECURITY RISKS PERFORM FULL WEBSITE BACKUP MAKE THE RIGHTCERTIFICATE CHOICE INSTALL AND TESTCERTIFICATES REMOVE MIXED CONTENT MAINTAIN CERTIFICATE COMPLIANCE REDIRECT HTTP TRAFFIC TO HTTPS IMPLEMENT AN AUTOMATED SCANNING SYSTEM SECURE YOUR COOKIES IMPLEMENT HTTP STRICT TRANSPORT SECURITY  参考\n 10 STEPS TO SWITCH FROM HTTP TO HTTPS  "}),e.add({id:26,href:"/study-network/docs/design/code/",title:"3.7 源码分析",section:"第三部分 设计与实现",content:"源码分析 #  "}),e.add({id:27,href:"/study-network/docs/basic/cxl/",title:"CXL",section:"第一部分 基础入门",content:"CXL #  Compute Express Link\n 以下是 CXL 和 PCIe 之间的一些区别：\n 带宽：CXL 提供比 PCIe 高得多的带宽。该标准的最新版本 CXL 3.0 每通道提供高达 64 GT/s 的带宽，是 PCIe 5.0 带宽的 2 倍。这使得 CXL 成为需要快速传输大量数据的应用程序的更好选择，例如人工智能和机器学习。 可扩展性：CXL 的设计比 PCIe 更具可扩展性。PCIe 仅限于最多 16 个通道，而 CXL 最多可支持 32 个通道。这意味着 CXL 可以用来连接更多的设备并提供比 PCIe 更多的带宽。 内存共享：CXL 支持设备之间的内存共享，使它们能够更有效地协同工作。这可以提高性能并减少通过互连在设备之间传输数据的需要。 兼容性：PCIe 是一种广泛采用的标准，与多种设备兼容，而 CXL 是一种较新的标准，仍在不断获得采用。这意味着 CXL 兼容设备可能需要一些时间才能广泛使用。成本：由于标准的附加功能和特性，支持 CXL 的设备可能比支持 PCIe 的设备更昂贵。  最终，CXL 和 PCIe 之间的选择将取决于应用程序的具体需求和要求，以及支持每个标准的设备的可用性和兼容性。\n"}),e.add({id:28,href:"/study-network/docs/basic/rdmd/ib/",title:"Infiniband",section:"RDMA",content:"Infiniband #   IPoIB #  Internet Protocol over InfiniBand\n  利用物理 IB 网络（包括服务器上的 IB 卡、IB 连接线、IB 交换机等）通过 IP 协议进行连接，并进行数据传输。\n  IPoIB 性能比 RDMA 通信方式性能要低，大多数应用都会采用 RDMA 方式获取高带宽低延时的收益，少数的关键应用会采用 IPoIB 方式通信。\n  IPoIB 设备能够配置为 datagram 和 connected 两种模式，前者提供不可靠的、无连接的链路，后者提供可靠的、有连接的链路。\n 在 datagram 模式下，queue pair 不允许报文大小超过 IB 链路层的 MTU 值，由于 IPoIB 头还包含了 4 字节，因此 IPoIB 的 MTU 值要小于 IB 链路层的 MTU 值。 在 connected 模式下，queue pair 允许发送比 IB 链路层更大的报文，理论上可以发送大小 65535 长度的报文。 connected 模式具有更好的性能，但是会消耗系统更多的内存。多数系统更关注性能，因此大多数场景下 IB 网口配置为 connected 模式。    ip link show type ipoib "}),e.add({id:29,href:"/study-network/docs/basic/pcie/",title:"PCIe",section:"第一部分 基础入门",content:"PCIe #  Peripheral Component Interconnect Express，简称 PCI-E，官方简称 PCIe\n是计算机总线的一个重要分支，它沿用既有的 PCI 编程概念及信号标准，并且构建了更加高速的串行通信系统标准。\n PCIe 拥有更快的速率，所以几乎取代了以往所有的内部总线（包括 AGP 和 PCI）。现在英特尔和 AMD 已采用单芯片组技术，取代原有的南桥／北桥方案。  发展历史 #   ISA (Industry Standard Architecture)  第一代 ISA 插槽出现在第一代 IBM PC XT 机型上（1981），作为现代 PC 的盘古之作，8 位的 ISA 提供了 4.77MB/s 的带宽（或传输率）。   MCA (Micro Channel Architecture) EISA (Extended Industry Standard Architecture) VLB (VESA Local Bus) PCI (Peripheral Component Interconnect)  Intel 在 1992 年提出 PCI（Peripheral Component Interconnect）总线协议，并召集其它的小伙伴组成了名为 PCI-SIG (PCI Special Interest Group)（PCI 特殊兴趣组 J）的企业联盟。从那以后这个组织就负责 PCI 和其继承者们（PCI-X 和 PCIe）的标准制定和推广。   PCI-X (Peripheral Component Interconnect eXtended) AGP (Accelerated Graphics Port) PCI Express (Peripheral Component Interconnect Express)  "}),e.add({id:30,href:"/study-network/docs/basic/rdmd/",title:"RDMA",section:"第一部分 基础入门",content:"RDMA #  分布式存储中常用的网络协议\n Infiniband（IB）：常用于 DPC 场景中的存储前端网络 RoCE：常用于存储后端网络 TCP/IP：常用于业务网络    传统的 TCP/IP 网络通信是通过内核发送消息，这种通信方式存在很高的数据移动和数据复制的开销。 RDMA (RemoteDirect Memory Access) 技术全称远程直接内存访问，就是为了解决网络传输中服务器端数据处理的延迟而产生的。  RDMA 技术能直接通过网络接口访问内存数据，无需操作系统内核的介入。这允许高吞吐、低延迟的网络通信，尤其适合在大规模并行计算机集群中使用。    目前有三种 RDMA 网络，分别是 Infiniband（IB）、RoCE (RDMA over Converged Ethernet)、iWARP  Infiniband 是一种专为 RDMA 设计的网络，从硬件级别保证可靠传输 ，技术先进，但是成本高昂  InfiniBand：设计之初就考虑了 RDMA，从硬件级别保证可靠传输，提供更高的带宽和更低的时延。但是成本高，需要支持 IB 网卡和交换机。   RoCE 和 iWARP 都是基于以太网的 RDMA 技术，这使高速、超低延时、极低 CPU 使用率的 RDMA 技术得以部署在目前使用最广泛的以太网上  RoCE：基于 Ethernet 做 RDMA，消耗的资源比 iWARP 少，支持的特性比 iWARP 多。  可以使用普通的以太网交换机，但是需要支持 RoCE 的网卡。   iWARP：基于 TCP 做 RDMA，利用 TCP 达到可靠传输。相比 RoCE，在大型组网的情况下，iWARP 的大量 TCP 连接会占用大量的内存资源，对系统规格要求更高。  可以使用普通的以太网交换机，但是需要支持 iWARP 的网卡。            InfiniBand iWARP RoCE     成本 高 中 低   性能 最好 稍差（受 TCP 影响） 与 InfiniBand 相当   稳定性 好 差 较好   交换机 IB 交换机 以太网交换机 以太网交换机    "}),e.add({id:31,href:"/study-network/docs/basic/rdmd/roce/",title:"RoCE 网络",section:"RDMA",content:"RoCE 网络 #  RoCE 协议有 RoCEv1 和 RoCEv2 两个版本\n RoCEv1 是基于以太网链路层实现的 RDMA 协议 (交换机需要支持 PFC 等流控技术，在物理层保证可靠传输)  2010 年 4 月，IBTA 发布了 RoCE，此标准是作为 Infiniband Architecture Specification 的附加件发布的，所以也称为 IBoE（InfiniBand over Ethernet）   RoCEv2 是以太网 TCP/IP 协议中 UDP 层实现， 引入 IP 解决了扩展性问题  "})})()